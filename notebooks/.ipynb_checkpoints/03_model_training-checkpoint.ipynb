{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69e3cd2",
   "metadata": {},
   "source": [
    "# Heart Failure Readmission Prediction Models\n",
    "\n",
    "This notebook builds and evaluates machine learning models to predict 30-day readmissions for heart failure patients.\n",
    "\n",
    "## Goals:\n",
    "1. Train multiple model types (Logistic Regression, Random Forest, XGBoost)\n",
    "2. Compare model performance using appropriate metrics\n",
    "3. Analyze feature importance\n",
    "4. Save the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import shap\n",
    "\n",
    "# Add the src directory to Python path\n",
    "notebook_dir = Path(r'c:/Users/tyagi/Desktop/heartbyte/notebooks')\n",
    "src_dir = notebook_dir.parent / 'src'\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "from data_loader import MIMICDataLoader\n",
    "from data_processing import preprocess_features, prepare_model_data, calculate_readmissions, clean_features_for_modeling\n",
    "from modeling import (\n",
    "    train_logistic_regression, train_random_forest, train_xgboost,\n",
    "    evaluate_model, plot_model_evaluation, save_model, \n",
    "    explain_model_with_shap, plot_shap_dependence\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df4e62",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data\n",
    "\n",
    "First, we'll load the heart failure patient data and preprocess it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 58976 admissions\n",
      "Loaded 46520 patients\n",
      "Loaded 651047 diagnoses\n",
      "Loaded 240095 procedures\n",
      "Found 78963 procedures for heart failure patients\n",
      "Found 10272 patients with heart failure\n",
      "These patients had 16756 admissions\n",
      "Training set: (13404, 30), Testing set: (3352, 30)\n",
      "Class distribution in training set: \n",
      "is_readmission\n",
      "0    0.902\n",
      "1    0.098\n",
      "Name: proportion, dtype: float64\n",
      "Training set: (13404, 30), Testing set: (3352, 30)\n",
      "Class distribution in training set: \n",
      "is_readmission\n",
      "0    0.902\n",
      "1    0.098\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load heart failure patient data\n",
    "loader = MIMICDataLoader()\n",
    "loader.load_data()\n",
    "hf_patients, hf_admissions, hf_diagnoses, hf_procedures = loader.filter_heart_failure_patients()\n",
    "\n",
    "# Calculate readmissions\n",
    "hf_admissions = calculate_readmissions(hf_admissions)\n",
    "\n",
    "# Create feature dataset (from 02_data_analysis.ipynb)\n",
    "def create_features(patients, admissions, diagnoses, procedures):\n",
    "    # Start with admissions data\n",
    "    features = admissions.copy()\n",
    "    \n",
    "    # Add patient demographics and calculate age\n",
    "    patient_features = patients[['subject_id']].copy()\n",
    "    \n",
    "    # Add gender if available\n",
    "    if 'gender' in patients.columns:\n",
    "        patient_features['gender'] = patients['gender']\n",
    "        features = features.merge(patient_features[['subject_id', 'gender']], \n",
    "                                on='subject_id', \n",
    "                                how='left')\n",
    "    \n",
    "    # Calculate age if DOB is available\n",
    "    if 'dob' in patients.columns and 'admittime' in admissions.columns:\n",
    "        # Convert dates to datetime\n",
    "        patient_features['dob'] = pd.to_datetime(patients['dob'])\n",
    "        features['admittime'] = pd.to_datetime(features['admittime'])\n",
    "        \n",
    "        # Calculate age using year difference then adjust for month and day\n",
    "        features = features.merge(patient_features[['subject_id', 'dob']], \n",
    "                                on='subject_id', \n",
    "                                how='left')\n",
    "        features['age'] = features.apply(lambda x: \n",
    "            (x['admittime'].year - x['dob'].year) - \n",
    "            ((x['admittime'].month, x['admittime'].day) < \n",
    "             (x['dob'].month, x['dob'].day)), axis=1)\n",
    "        \n",
    "        # Drop temporary DOB column\n",
    "        features = features.drop('dob', axis=1)\n",
    "    \n",
    "    # Calculate length of stay if discharge time is available\n",
    "    if 'dischtime' in features.columns:\n",
    "        features['dischtime'] = pd.to_datetime(features['dischtime'])\n",
    "        features['length_of_stay'] = (features['dischtime'] - \n",
    "                                     features['admittime']).dt.total_seconds() / (24*60*60)\n",
    "    \n",
    "    # Calculate admission count and time since last admission\n",
    "    features['admission_count'] = features.groupby('subject_id').cumcount() + 1\n",
    "    \n",
    "    if 'dischtime' in features.columns:\n",
    "        features['prev_dischtime'] = features.groupby('subject_id')['dischtime'].shift(1)\n",
    "        features['days_since_last_admission'] = (features['admittime'] - \n",
    "                                                pd.to_datetime(features['prev_dischtime'])).dt.total_seconds() / (24*60*60)\n",
    "    \n",
    "    # Count comorbidities per admission\n",
    "    if 'hadm_id' in diagnoses.columns:\n",
    "        comorbidity_counts = diagnoses.groupby('hadm_id').size().reset_index(name='num_diagnoses')\n",
    "        features = features.merge(comorbidity_counts, on='hadm_id', how='left')\n",
    "    \n",
    "    # Count procedures per admission if available\n",
    "    if procedures is not None and 'hadm_id' in procedures.columns:\n",
    "        procedure_counts = procedures.groupby('hadm_id').size().reset_index(name='num_procedures')\n",
    "        features = features.merge(procedure_counts, on='hadm_id', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    numerical_columns = ['length_of_stay', 'days_since_last_admission', 'num_diagnoses', 'num_procedures']\n",
    "    for col in numerical_columns:\n",
    "        if col in features.columns:\n",
    "            features[col] = features[col].fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create feature dataset\n",
    "features_df = create_features(hf_patients, hf_admissions, hf_diagnoses, hf_procedures)\n",
    "\n",
    "# Clean features by removing datetime and non-numeric columns\n",
    "# Use the module function from data_processing\n",
    "features_df_clean = clean_features_for_modeling(features_df)\n",
    "\n",
    "# Print info about the cleaning process\n",
    "datetime_cols = [col for col in features_df.columns if pd.api.types.is_datetime64_any_dtype(features_df[col])]\n",
    "object_cols = [col for col in features_df.columns if pd.api.types.is_object_dtype(features_df[col])]\n",
    "other_non_numeric = [col for col in features_df.columns if not pd.api.types.is_numeric_dtype(features_df[col]) \n",
    "                    and col not in datetime_cols and col not in object_cols]\n",
    "\n",
    "if datetime_cols:\n",
    "    print(f\"Removed datetime columns: {datetime_cols}\")\n",
    "if object_cols:\n",
    "    print(f\"Removed object columns: {object_cols}\")\n",
    "if other_non_numeric:\n",
    "    print(f\"Removed other non-numeric columns: {other_non_numeric}\")\n",
    "\n",
    "# Display the columns that remained after cleaning\n",
    "print(f\"\\nRemaining features after cleaning: {features_df_clean.shape[1]}\")\n",
    "print(f\"Feature columns: {features_df_clean.columns.tolist()}\")\n",
    "\n",
    "# Prepare data for modeling using the cleaned features\n",
    "X_train, X_test, y_train, y_test = prepare_model_data(\n",
    "    features_df_clean, \n",
    "    target_col='is_readmission', \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Training set: {X_train.shape}, Testing set: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1b922",
   "metadata": {},
   "source": [
    "## 2. Train and Evaluate Models\n",
    "\n",
    "We'll train three models with different complexity levels:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest\n",
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d073503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names for importance analysis\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c31511",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705403b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast DatetimeArray to dtype float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train logistic regression model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Logistic Regression model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m lr_model = \u001b[43mtrain_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[32m      6\u001b[39m lr_results = evaluate_model(lr_model, X_test, y_test, feature_names)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tyagi\\Desktop\\heartbyte\\src\\modeling.py:49\u001b[39m, in \u001b[36mtrain_logistic_regression\u001b[39m\u001b[34m(X_train, y_train, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     48\u001b[39m model = LogisticRegression(**params)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:973\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    972\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    975\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:179\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np.ndarray):\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    182\u001b[39m     values = _astype_nansafe(values, dtype, copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:739\u001b[39m, in \u001b[36mDatetimeArray.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, PeriodDtype):\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_period(freq=dtype.freq)\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDatetimeLikeArrayMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\heartbyte\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:494\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.astype\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmM\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype != dtype) \u001b[38;5;129;01mor\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# disallow conversion between datetime/timedelta,\u001b[39;00m\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# and conversions for any datetimelike to float\u001b[39;00m\n\u001b[32m    493\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot cast \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(\u001b[38;5;28mself\u001b[39m, dtype=dtype)\n",
      "\u001b[31mTypeError\u001b[39m: Cannot cast DatetimeArray to dtype float64"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "lr_model = train_logistic_regression(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "lr_results = evaluate_model(lr_model, X_test, y_test, feature_names)\n",
    "results['logistic_regression'] = lr_results\n",
    "\n",
    "# Display evaluation\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "print(f\"Accuracy: {lr_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {lr_results['precision']:.4f}\")\n",
    "print(f\"Recall: {lr_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {lr_results['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {lr_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Plot evaluation\n",
    "plot_model_evaluation(lr_results, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad94a15",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = train_random_forest(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "rf_results = evaluate_model(rf_model, X_test, y_test, feature_names)\n",
    "results['random_forest'] = rf_results\n",
    "\n",
    "# Display evaluation\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(f\"Accuracy: {rf_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {rf_results['precision']:.4f}\")\n",
    "print(f\"Recall: {rf_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {rf_results['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {rf_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Plot evaluation\n",
    "plot_model_evaluation(rf_results, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd9601",
   "metadata": {},
   "source": [
    "### 2.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = train_xgboost(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "xgb_results = evaluate_model(xgb_model, X_test, y_test, feature_names)\n",
    "results['xgboost'] = xgb_results\n",
    "\n",
    "# Display evaluation\n",
    "print(\"\\nXGBoost Performance:\")\n",
    "print(f\"Accuracy: {xgb_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {xgb_results['precision']:.4f}\")\n",
    "print(f\"Recall: {xgb_results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {xgb_results['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {xgb_results['roc_auc']:.4f}\")\n",
    "\n",
    "# Plot evaluation\n",
    "plot_model_evaluation(xgb_results, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38f260",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models based on key metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "comparison_data = {\n",
    "    'Logistic Regression': [results['logistic_regression'][m] for m in metrics],\n",
    "    'Random Forest': [results['random_forest'][m] for m in metrics],\n",
    "    'XGBoost': [results['xgboost'][m] for m in metrics]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data, index=metrics)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Model Performance Comparison', fontsize=16)\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1c787",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances across models\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Get top 10 features from each model\n",
    "feature_data = {}\n",
    "top_n = 10\n",
    "\n",
    "if 'feature_importance' in results['logistic_regression']:\n",
    "    lr_fi = results['logistic_regression']['feature_importance']\n",
    "    feature_data['Logistic Regression'] = {\n",
    "        'names': lr_fi['names'][:top_n], \n",
    "        'values': lr_fi['values'][:top_n]\n",
    "    }\n",
    "\n",
    "if 'feature_importance' in results['random_forest']:\n",
    "    rf_fi = results['random_forest']['feature_importance']\n",
    "    feature_data['Random Forest'] = {\n",
    "        'names': rf_fi['names'][:top_n], \n",
    "        'values': rf_fi['values'][:top_n]\n",
    "    }\n",
    "\n",
    "if 'feature_importance' in results['xgboost']:\n",
    "    xgb_fi = results['xgboost']['feature_importance']\n",
    "    feature_data['XGBoost'] = {\n",
    "        'names': xgb_fi['names'][:top_n], \n",
    "        'values': xgb_fi['values'][:top_n]\n",
    "    }\n",
    "\n",
    "# Plot top features for each model\n",
    "num_models = len(feature_data)\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(16, 8))\n",
    "\n",
    "for i, (model_name, data) in enumerate(feature_data.items()):\n",
    "    ax = axes[i] if num_models > 1 else axes\n",
    "    sns.barplot(x=data['values'], y=data['names'], ax=ax)\n",
    "    ax.set_title(f\"{model_name} - Top {top_n} Features\")\n",
    "    ax.set_xlabel('Importance')\n",
    "    if i > 0 and num_models > 1:\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e663d",
   "metadata": {},
   "source": [
    "## 5. Save Best Model\n",
    "\n",
    "Based on our evaluation metrics, we'll save the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on F1 score (balances precision and recall)\n",
    "f1_scores = {\n",
    "    'Logistic Regression': results['logistic_regression']['f1'],\n",
    "    'Random Forest': results['random_forest']['f1'],\n",
    "    'XGBoost': results['xgboost']['f1']\n",
    "}\n",
    "\n",
    "best_model_name = max(f1_scores, key=f1_scores.get)\n",
    "print(f\"Best model based on F1 score: {best_model_name} (F1 = {f1_scores[best_model_name]:.4f})\")\n",
    "\n",
    "# Save the best model\n",
    "models = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model\n",
    "}\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = model_dir / f\"readmission_{best_model_name.lower().replace(' ', '_')}_{timestamp}.joblib\"\n",
    "\n",
    "save_model(best_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71868e",
   "metadata": {},
   "source": [
    "## 6. Model Interpretability with SHAP\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to interpret the models and understand feature contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82aec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP explanations for the best model\n",
    "print(f\"Generating SHAP explanations for {best_model_name}...\")\n",
    "\n",
    "# Sample data if it's large to speed up computation\n",
    "sample_size = 500 if X_test.shape[0] > 500 else X_test.shape[0]\n",
    "\n",
    "# Create SHAP explainer and get values\n",
    "explainer, shap_values = explain_model_with_shap(\n",
    "    best_model, \n",
    "    X_test, \n",
    "    sample_size=sample_size, \n",
    "    plot_type='summary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate beeswarm plot to show detailed feature impact\n",
    "explain_model_with_shap(best_model, X_test, sample_size=sample_size, plot_type='beeswarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bar plot for absolute SHAP value importance\n",
    "explain_model_with_shap(best_model, X_test, sample_size=sample_size, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dacff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top features from SHAP analysis\n",
    "feature_names = X_test.columns.tolist()\n",
    "mean_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.DataFrame(list(zip(feature_names, mean_shap_values)), \n",
    "                                 columns=['feature', 'shap_importance'])\n",
    "feature_importance = feature_importance.sort_values('shap_importance', ascending=False).reset_index(drop=True)\n",
    "top_features = feature_importance['feature'][:5].tolist()\n",
    "\n",
    "print(\"Top 5 features by SHAP importance:\")\n",
    "print(feature_importance.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dependence plots for the most important features\n",
    "for feature in top_features[:3]:  # Show top 3 features\n",
    "    plot_shap_dependence(shap_values, X_test, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a waterfall plot for a specific sample\n",
    "sample_idx = 0  # First sample\n",
    "explain_model_with_shap(best_model, X_test.iloc[[sample_idx]], plot_type='waterfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b78aa",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we trained and evaluated three different models for predicting 30-day readmissions for heart failure patients:\n",
    "- Logistic Regression (baseline)\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "\n",
    "Key observations:\n",
    "1. Performance comparison between models\n",
    "2. Most important features for predicting readmissions based on both model-specific importance and SHAP values\n",
    "3. Detailed interpretation of how features affect predictions\n",
    "4. Areas for model improvement\n",
    "\n",
    "Next steps:\n",
    "1. Hyperparameter tuning for the best model\n",
    "2. Feature engineering to focus on high-impact variables\n",
    "3. Model deployment as an inference pipeline\n",
    "4. Clinical validation of findings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
